# ğŸ“¡ ITmedia ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼†Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã‚¹ã‚¯ãƒªãƒ—ãƒˆ

## ğŸ“˜ æ¦‚è¦

ITmediaã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰è¨˜äº‹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ»URLãƒ»å…¬é–‹æ—¥æ™‚ï¼‰ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼†Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è‡ªå‹•è¨˜éŒ²ã™ã‚‹Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

- é‡è¤‡è¨˜äº‹ã¯è‡ªå‹•æ’é™¤
- æ¯æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ­ã‚°ã¨ã—ã¦ã‚‚æ´»ç”¨å¯èƒ½
- ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºæ¸ˆã¿

---

## âœ¨ ä½¿ç”¨æ–¹æ³•

### 1. å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install -r requirements.txt
```

### 2. Google Cloudã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨­å®š

#### âœ… ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆ

1. [Google Cloud Console](https://console.cloud.google.com/) ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã¾ãŸã¯é¸æŠ
3. å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ â†’ã€ŒAPIã¨ã‚µãƒ¼ãƒ“ã‚¹ã€â†’ã€Œèªè¨¼æƒ…å ±ã€
4. ã€Œèªè¨¼æƒ…å ±ã‚’ä½œæˆã€â†’ã€Œã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã€
5. ä»»æ„ã®åå‰ã§ä½œæˆï¼ˆä¾‹ï¼špython-scraperï¼‰
6. ã€Œéµã€ã‚¿ãƒ– â†’ã€Œéµã‚’è¿½åŠ ã€â†’ã€ŒJSONã€å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

âœ… ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ«ãƒ¼ãƒˆã« `credentials.json` ã¨ã—ã¦ä¿å­˜

#### âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå´ã®è¨­å®š

1. å¯¾è±¡ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
2. å³ä¸Šã€Œå…±æœ‰ã€â†’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¿½åŠ ã—ã€Œç·¨é›†è€…ã€ã«è¨­å®š

---

## ğŸƒâ€â™‚ï¸ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

```bash
python3 main.py
```

---

## ğŸ§¹ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã¨å½¹å‰²

### `main.py`

- å‡¦ç†ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
- è¨˜äº‹å–å¾—ãƒ»é‡è¤‡é™¤å¤–ãƒ»CSVä¿å­˜ãƒ»ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€ä¿¡ã®ä¸€é€£ã®æµã‚Œã‚’å®Ÿè¡Œ

### `scraper.py`

- ITmediaã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
- ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å…¬é–‹æ—¥ï¼ˆä¾‹ï¼š4æœˆ24æ—¥ 18æ™‚00åˆ†ï¼‰ã‚’æŠ½å‡º

### `spreadsheet.py`

- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜äº‹ã‚’1ä»¶ãšã¤æ›¸ãè¾¼ã‚€
- `gspread`ã¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§OAuthèªè¨¼

---

## ğŸ”„ main.py å®Ÿè¡Œãƒ•ãƒ­ãƒ¼å›³

```mermaid
graph TD
    main_py["main.py å®Ÿè¡Œ"] --> get_articles["articles = scrape_itmedia_news()"]
    get_articles -->|scrape_itmedia_news() å‘¼ã³å‡ºã—| scraper["scraper.py å®Ÿè¡Œ"]
    scraper --> collect["è¨˜äº‹ä¸€è¦§ã‚’å–å¾—ã—ãƒªã‚¹ãƒˆã«æ ¼ç´"]
    get_articles --> get_existing["get_existing_titles_and_urls()"]
    get_existing --> read_csv["æ—¢å­˜CSVã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ãƒ»URLã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ"]
    get_articles --> dedup["remove_duplicates()"]
    dedup --> filter["é‡è¤‡ã—ãªã„è¨˜äº‹ã®ã¿ã‚’æŠ½å‡º"]
    filter --> save_csv["pandas.DataFrame ã«å¤‰æ›ã—ã¦CSVä¿å­˜"]
    save_csv --> spreadsheet["write_to_spreadsheet()"]
    spreadsheet --> write_gsheet["Google Spreadsheet ã«æ›¸ãè¾¼ã¿"]
```

---

## ğŸ“‚ å‡ºåŠ›ä¾‹

- `data/itmedia_news_YYYYMMDD.csv`
![csvãƒ•ã‚¡ã‚¤ãƒ«](./images/CSVFile.png)
- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«1è¡Œãšã¤è¿½åŠ 
![ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ](./images/ITmedia_SpreadSheet.png)

---

## ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨æ³¨æ„ç‚¹

- `credentials.json` ã¯**çµ¶å¯¾ã«Gitã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã“ã¨ï¼**
- `.gitignore` ã«ä»¥ä¸‹ã‚’è¿½åŠ ï¼š
  ```gitignore
  credentials.json
  *.json
  ```

---

## ğŸ‘¨â€ğŸ’¼ ä½œè€…

Ryoma Ueda  
ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ»å­¦ç¿’ãƒ­ã‚°ã¨ã—ã¦GitHubã§å…¬é–‹ä¸­  
ï¼ˆå‰¯æ¥­ãƒ»è‡ªå‹•åŒ–ãƒ»Pythonãƒ„ãƒ¼ãƒ«é–‹ç™ºã«èˆˆå‘³ã‚ã‚Šï¼‰

---

